{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47882/518899274.py:3: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538456841/work/c10/core/TensorImpl.h:1925.)\n",
      "  _ = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"c\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('c',))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"c\"])\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5)  # shape[channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "weights\n",
    "# img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8451,  0.2919,  0.2824, -1.6726, -1.3670],\n",
       "          [ 0.6456,  0.0890,  1.2987, -0.2207,  0.2485],\n",
       "          [ 1.0622,  0.7560, -1.0527,  0.1295, -0.1111],\n",
       "          [-1.4482,  0.0640,  0.3154, -0.4700, -1.1466],\n",
       "          [-0.4352,  0.9409, -0.0738, -1.0342, -0.0613]],\n",
       "\n",
       "         [[ 0.0167, -0.2286, -1.3227, -0.0913,  0.2351],\n",
       "          [ 0.9751,  0.4635, -0.5464,  1.2534,  0.2755],\n",
       "          [ 0.1288, -2.5053, -2.0056,  0.2155,  1.9605],\n",
       "          [-0.0458, -0.8030,  1.6390,  0.0700, -0.7103],\n",
       "          [ 1.6720,  0.2473,  0.7807, -0.5719,  1.6353]],\n",
       "\n",
       "         [[ 0.2090, -0.0593, -2.1235, -0.6354,  0.6355],\n",
       "          [-0.0666,  2.4667, -0.0954,  1.3738,  0.3804],\n",
       "          [ 0.6903, -0.8842,  0.1841,  0.5826, -0.9782],\n",
       "          [ 0.1314, -1.4169, -1.5618, -0.0302,  1.0518],\n",
       "          [-0.9500, -0.3659, -1.9563, -0.1227, -1.9489]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4660,  0.0056, -0.2504,  0.5091,  0.9817],\n",
       "          [-0.0349, -1.2229,  0.5041,  0.6071, -0.0502],\n",
       "          [-0.1452,  0.5569, -0.3847,  0.0030,  0.2028],\n",
       "          [ 1.3086,  0.6868,  0.0290,  0.4764, -1.3092],\n",
       "          [-0.5469,  0.3982,  0.7688, -0.2602, -1.9492]],\n",
       "\n",
       "         [[ 1.2738,  1.3184,  1.2824, -0.2355,  0.5615],\n",
       "          [-1.0484, -0.1521,  0.3980,  0.6093, -1.9455],\n",
       "          [ 0.2131, -1.3253, -0.2064, -0.2635,  0.2889],\n",
       "          [-0.6407,  0.3080,  1.3129,  0.8324,  0.0144],\n",
       "          [ 0.8225,  1.9774, -0.3932, -0.3689,  1.0777]],\n",
       "\n",
       "         [[-0.0149, -0.0272, -0.1766,  1.4538,  0.8283],\n",
       "          [-0.7984, -0.2247, -0.1357, -0.6567,  0.3551],\n",
       "          [-0.3246, -0.5381,  0.9341, -0.0339, -1.2520],\n",
       "          [ 0.5391,  0.6327,  2.1744,  0.3080,  0.9784],\n",
       "          [-1.3344,  1.0989, -0.6924, -0.8475, -1.3630]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-3 represent the 3rd from the last demension, now is channel, usual use this to converted RGB to gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_navie = img_t.mean(-3)\n",
    "img_gray_navie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1978,  0.0639,  0.0964, -1.0549, -0.7361],\n",
       "        [ 0.6751,  0.4716,  0.5231, -0.6193, -0.0021],\n",
       "        [ 0.6630, -0.1405, -0.4087,  0.5052, -0.1133],\n",
       "        [-0.2274, -0.0105,  0.0940,  0.7019,  0.1376],\n",
       "        [ 0.2410,  0.4988,  0.5725, -0.0235, -0.2560]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_navie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)\n",
    "batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2126],\n",
       "        [0.7152],\n",
       "        [0.0722]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weightd = weights.unsqueeze(-1)\n",
    "unsqueezed_weightd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2126]],\n",
       "\n",
       "        [[0.7152]],\n",
       "\n",
       "        [[0.0722]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weightd = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "unsqueezed_weightd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7212,  1.8448,  0.4451,  0.6360, -0.2658],\n",
       "         [-0.4818,  0.2013,  0.2868, -0.1150,  1.2647],\n",
       "         [ 0.4788,  0.4130,  0.0868,  1.0954, -0.2706],\n",
       "         [ 0.7543,  0.6442, -0.7009, -0.7020, -1.0852],\n",
       "         [ 0.5316, -1.3757,  0.0088,  0.3538, -0.5783]],\n",
       "\n",
       "        [[-1.2277, -0.2256, -0.2802, -0.5172, -2.3308],\n",
       "         [-0.5142, -2.1828, -0.8337,  1.3794,  0.1203],\n",
       "         [-0.0687, -0.3638, -0.6658,  0.7286,  0.1374],\n",
       "         [ 0.5056,  1.4695,  2.9591,  0.6710,  0.7015],\n",
       "         [ 0.1561,  0.1887,  0.8208, -1.0239, -1.9043]],\n",
       "\n",
       "        [[-1.3203,  1.4553,  0.5082,  0.0653,  2.0771],\n",
       "         [ 1.2853, -1.0630, -1.1397, -0.1577,  2.6100],\n",
       "         [-0.3586, -0.7229,  0.7332, -1.6746,  0.8347],\n",
       "         [ 0.8298, -1.7016, -2.0686,  0.1078,  0.7827],\n",
       "         [ 1.0556, -0.6898,  0.3499,  0.7913,  0.9131]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5333e-01,  3.9220e-01,  9.4628e-02,  1.3521e-01, -5.6507e-02],\n",
       "         [-1.0243e-01,  4.2803e-02,  6.0969e-02, -2.4439e-02,  2.6888e-01],\n",
       "         [ 1.0178e-01,  8.7804e-02,  1.8455e-02,  2.3289e-01, -5.7523e-02],\n",
       "         [ 1.6037e-01,  1.3696e-01, -1.4901e-01, -1.4924e-01, -2.3072e-01],\n",
       "         [ 1.1302e-01, -2.9247e-01,  1.8762e-03,  7.5216e-02, -1.2294e-01]],\n",
       "\n",
       "        [[-8.7807e-01, -1.6135e-01, -2.0042e-01, -3.6987e-01, -1.6670e+00],\n",
       "         [-3.6774e-01, -1.5611e+00, -5.9626e-01,  9.8656e-01,  8.6041e-02],\n",
       "         [-4.9145e-02, -2.6020e-01, -4.7617e-01,  5.2107e-01,  9.8294e-02],\n",
       "         [ 3.6157e-01,  1.0510e+00,  2.1164e+00,  4.7991e-01,  5.0169e-01],\n",
       "         [ 1.1164e-01,  1.3498e-01,  5.8706e-01, -7.3233e-01, -1.3620e+00]],\n",
       "\n",
       "        [[-9.5325e-02,  1.0508e-01,  3.6689e-02,  4.7179e-03,  1.4997e-01],\n",
       "         [ 9.2800e-02, -7.6749e-02, -8.2287e-02, -1.1389e-02,  1.8844e-01],\n",
       "         [-2.5891e-02, -5.2194e-02,  5.2939e-02, -1.2091e-01,  6.0264e-02],\n",
       "         [ 5.9912e-02, -1.2285e-01, -1.4935e-01,  7.7864e-03,  5.6508e-02],\n",
       "         [ 7.6212e-02, -4.9801e-02,  2.5263e-02,  5.7135e-02,  6.5927e-02]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = img_t * unsqueezed_weightd\n",
    "img_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7966e-01,  6.2060e-02,  6.0029e-02, -3.5559e-01, -2.9063e-01],\n",
       "          [ 1.3726e-01,  1.8922e-02,  2.7610e-01, -4.6930e-02,  5.2832e-02],\n",
       "          [ 2.2583e-01,  1.6072e-01, -2.2379e-01,  2.7523e-02, -2.3627e-02],\n",
       "          [-3.0790e-01,  1.3617e-02,  6.7063e-02, -9.9930e-02, -2.4376e-01],\n",
       "          [-9.2523e-02,  2.0004e-01, -1.5681e-02, -2.1986e-01, -1.3041e-02]],\n",
       "\n",
       "         [[ 1.1917e-02, -1.6347e-01, -9.4601e-01, -6.5314e-02,  1.6813e-01],\n",
       "          [ 6.9736e-01,  3.3148e-01, -3.9081e-01,  8.9643e-01,  1.9703e-01],\n",
       "          [ 9.2153e-02, -1.7918e+00, -1.4344e+00,  1.5411e-01,  1.4021e+00],\n",
       "          [-3.2763e-02, -5.7428e-01,  1.1722e+00,  5.0038e-02, -5.0798e-01],\n",
       "          [ 1.1958e+00,  1.7684e-01,  5.5832e-01, -4.0901e-01,  1.1696e+00]],\n",
       "\n",
       "         [[ 1.5093e-02, -4.2842e-03, -1.5332e-01, -4.5878e-02,  4.5881e-02],\n",
       "          [-4.8099e-03,  1.7810e-01, -6.8852e-03,  9.9191e-02,  2.7468e-02],\n",
       "          [ 4.9840e-02, -6.3837e-02,  1.3290e-02,  4.2060e-02, -7.0628e-02],\n",
       "          [ 9.4864e-03, -1.0230e-01, -1.1276e-01, -2.1816e-03,  7.5941e-02],\n",
       "          [-6.8590e-02, -2.6420e-02, -1.4124e-01, -8.8619e-03, -1.4071e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 9.9068e-02,  1.1888e-03, -5.3241e-02,  1.0823e-01,  2.0871e-01],\n",
       "          [-7.4202e-03, -2.6000e-01,  1.0717e-01,  1.2906e-01, -1.0677e-02],\n",
       "          [-3.0873e-02,  1.1839e-01, -8.1792e-02,  6.4651e-04,  4.3106e-02],\n",
       "          [ 2.7822e-01,  1.4602e-01,  6.1717e-03,  1.0128e-01, -2.7833e-01],\n",
       "          [-1.1627e-01,  8.4650e-02,  1.6344e-01, -5.5311e-02, -4.1441e-01]],\n",
       "\n",
       "         [[ 9.1104e-01,  9.4289e-01,  9.1719e-01, -1.6844e-01,  4.0161e-01],\n",
       "          [-7.4983e-01, -1.0878e-01,  2.8468e-01,  4.3580e-01, -1.3914e+00],\n",
       "          [ 1.5242e-01, -9.4784e-01, -1.4763e-01, -1.8845e-01,  2.0663e-01],\n",
       "          [-4.5824e-01,  2.2025e-01,  9.3902e-01,  5.9531e-01,  1.0300e-02],\n",
       "          [ 5.8827e-01,  1.4142e+00, -2.8124e-01, -2.6382e-01,  7.7078e-01]],\n",
       "\n",
       "         [[-1.0782e-03, -1.9602e-03, -1.2753e-02,  1.0497e-01,  5.9806e-02],\n",
       "          [-5.7648e-02, -1.6223e-02, -9.7988e-03, -4.7414e-02,  2.5635e-02],\n",
       "          [-2.3433e-02, -3.8852e-02,  6.7442e-02, -2.4484e-03, -9.0398e-02],\n",
       "          [ 3.8922e-02,  4.5678e-02,  1.5699e-01,  2.2236e-02,  7.0643e-02],\n",
       "          [-9.6341e-02,  7.9338e-02, -4.9994e-02, -6.1188e-02, -9.8411e-02]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights = batch_t * unsqueezed_weightd\n",
    "batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1978,  0.0639,  0.0964, -1.0549, -0.7361],\n",
       "        [ 0.6751,  0.4716,  0.5231, -0.6193, -0.0021],\n",
       "        [ 0.6630, -0.1405, -0.4087,  0.5052, -0.1133],\n",
       "        [-0.2274, -0.0105,  0.0940,  0.7019,  0.1376],\n",
       "        [ 0.2410,  0.4988,  0.5725, -0.0235, -0.2560]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "img_gray_navie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weightd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum(\"...chw,c->...hw\", img_t, weights)\n",
    "img_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted_fancy = torch.einsum(\"...chw,c->...hw\", batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "batch_named = batch_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"channels\"])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum(\"channels\")\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum(\"channels\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
