{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "from util import XyzTuple, xyz2irc\n",
    "from util import enumerateWithEstimate\n",
    "from logconf import logging\n",
    "\n",
    "from disk import getCache\n",
    "from dsets import PrepcacheLunaDataset, getCtSampleSize\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "MaskTuple = namedtuple(\n",
    "    \"MaskTuple\",\n",
    "    \"raw_dense_mask, dense_mask, body_mask, air_mask, raw_candidate_mask, candidate_mask, lung_mask, neg_mask, pos_mask\",\n",
    ")\n",
    "\n",
    "CandidateInfoTuple = namedtuple(\n",
    "    \"CandidateInfoTuple\",\n",
    "    \"isNodule_bool, hasAnnotation_bool, isMal_bool, diameter_mm, series_uid, center_xyz\",\n",
    ")\n",
    "\n",
    "ct_path = \"/media/lim/Elements/3723295\"\n",
    "annotations_path = os.path.join(ct_path, \"annotations_with_malignancy.csv\")\n",
    "candidate_path = os.path.join(ct_path, \"candidates.csv\")\n",
    "ctscan_path = os.path.join(ct_path, \"subset\", \"subset0\")\n",
    "raw_cache = getCache(\"raw\")\n",
    "\n",
    "\n",
    "@functools.lru_cache(1)\n",
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    # We construct a set with all series_uids that are present on disk.\n",
    "    # This will let us use the data, even if we haven't downloaded all of\n",
    "    # the subsets yet.\n",
    "    mhd_list = glob.glob(os.path.join(ctscan_path, \"*.mhd\"))\n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    candidateInfo_list = []\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            # print(series_uid)\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "            # isMal_bool = row[5]\n",
    "            # print(isMal_bool)\n",
    "            isMal_bool = {\"0.0\": False, \"1.0\": True}[row[5]]\n",
    "\n",
    "            candidateInfo_list.append(\n",
    "                CandidateInfoTuple(\n",
    "                    True,\n",
    "                    True,\n",
    "                    isMal_bool,\n",
    "                    annotationDiameter_mm,\n",
    "                    series_uid,\n",
    "                    annotationCenter_xyz,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    with open(candidate_path, \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule_bool = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            if not isNodule_bool:\n",
    "                candidateInfo_list.append(\n",
    "                    CandidateInfoTuple(\n",
    "                        False,\n",
    "                        False,\n",
    "                        False,\n",
    "                        0.0,\n",
    "                        series_uid,\n",
    "                        candidateCenter_xyz,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CandidateInfoList = getCandidateInfoList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56928"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CandidateInfoList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@functools.lru_cache(1)\n",
    "def getCandidateInfoDict(requireOnDisk_bool=True):\n",
    "    candidateInfo_list = getCandidateInfoList(requireOnDisk_bool)\n",
    "    candidateInfo_dict = {}\n",
    "\n",
    "    for candidateInfo_tup in candidateInfo_list:\n",
    "        candidateInfo_dict.setdefault(candidateInfo_tup.series_uid, []).append(\n",
    "            candidateInfo_tup\n",
    "        )\n",
    "\n",
    "    return candidateInfo_dict\n",
    "\n",
    "\n",
    "CandidateInfoDict = getCandidateInfoDict()\n",
    "len(CandidateInfoDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            \"/media/lim/Elements/3723295/subset/subset0/{}.mhd\".format(series_uid)\n",
    "        )[0]\n",
    "\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        self.hu_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
    "\n",
    "        candidateInfo_list = getCandidateInfoDict()[self.series_uid]\n",
    "\n",
    "        self.positiveInfo_list = [\n",
    "            candidate_tup\n",
    "            for candidate_tup in candidateInfo_list\n",
    "            if candidate_tup.isNodule_bool\n",
    "        ]\n",
    "        self.positive_mask = self.buildAnnotationMask(self.positiveInfo_list)\n",
    "        self.positive_indexes = (\n",
    "            self.positive_mask.sum(axis=(1, 2)).nonzero()[0].tolist()\n",
    "        )\n",
    "\n",
    "    def buildAnnotationMask(self, positiveInfo_list, threshold_hu=-700):\n",
    "        boundingBox_a = np.zeros_like(self.hu_a, dtype=np.bool)\n",
    "\n",
    "        for candidateInfo_tup in positiveInfo_list:\n",
    "            center_irc = xyz2irc(\n",
    "                candidateInfo_tup.center_xyz,\n",
    "                self.origin_xyz,\n",
    "                self.vxSize_xyz,\n",
    "                self.direction_a,\n",
    "            )\n",
    "            ci = int(center_irc.index)\n",
    "            cr = int(center_irc.row)\n",
    "            cc = int(center_irc.col)\n",
    "\n",
    "            index_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci + index_radius, cr, cc] > threshold_hu\n",
    "                    and self.hu_a[ci - index_radius, cr, cc] > threshold_hu\n",
    "                ):\n",
    "                    index_radius += 1\n",
    "            except IndexError:\n",
    "                index_radius -= 1\n",
    "\n",
    "            row_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr + row_radius, cc] > threshold_hu\n",
    "                    and self.hu_a[ci, cr - row_radius, cc] > threshold_hu\n",
    "                ):\n",
    "                    row_radius += 1\n",
    "            except IndexError:\n",
    "                row_radius -= 1\n",
    "\n",
    "            col_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr, cc + col_radius] > threshold_hu\n",
    "                    and self.hu_a[ci, cr, cc - col_radius] > threshold_hu\n",
    "                ):\n",
    "                    col_radius += 1\n",
    "            except IndexError:\n",
    "                col_radius -= 1\n",
    "\n",
    "            # assert index_radius > 0, repr([candidateInfo_tup.center_xyz, center_irc, self.hu_a[ci, cr, cc]])\n",
    "            # assert row_radius > 0\n",
    "            # assert col_radius > 0\n",
    "\n",
    "            boundingBox_a[\n",
    "                ci - index_radius : ci + index_radius + 1,\n",
    "                cr - row_radius : cr + row_radius + 1,\n",
    "                cc - col_radius : cc + col_radius + 1,\n",
    "            ] = True\n",
    "\n",
    "        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n",
    "\n",
    "        return mask_a\n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        center_irc = xyz2irc(\n",
    "            center_xyz, self.origin_xyz, self.vxSize_xyz, self.direction_a\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis] / 2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr(\n",
    "                [\n",
    "                    self.series_uid,\n",
    "                    center_xyz,\n",
    "                    self.origin_xyz,\n",
    "                    self.vxSize_xyz,\n",
    "                    center_irc,\n",
    "                    axis,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if start_ndx < 0:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "        pos_chunk = self.positive_mask[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, pos_chunk, center_irc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(1, typed=True)\n",
    "def getCt(series_uid):\n",
    "    return Ct(series_uid)\n",
    "\n",
    "\n",
    "@raw_cache.memoize(typed=True)\n",
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, pos_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    ct_chunk.clip(-1000, 1000, ct_chunk)\n",
    "    return ct_chunk, pos_chunk, center_irc\n",
    "\n",
    "\n",
    "@raw_cache.memoize(typed=True)\n",
    "def getCtSampleSize(series_uid):\n",
    "    ct = Ct(series_uid)\n",
    "    return int(ct.hu_a.shape[0]), ct.positive_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool = np.bool_\n",
    "\n",
    "\n",
    "class Luna2dSegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        val_stride=0,\n",
    "        isValSet_bool=None,\n",
    "        series_uid=None,\n",
    "        contextSlices_count=3,\n",
    "        fullCt_bool=False,\n",
    "    ):\n",
    "        self.contextSlices_count = contextSlices_count\n",
    "        self.fullCt_bool = fullCt_bool\n",
    "\n",
    "        if series_uid:\n",
    "            self.series_list = [series_uid]\n",
    "        else:\n",
    "            self.series_list = sorted(getCandidateInfoDict().keys())\n",
    "\n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.series_list = self.series_list[::val_stride]\n",
    "            assert self.series_list\n",
    "        elif val_stride > 0:\n",
    "            del self.series_list[::val_stride]\n",
    "            assert self.series_list\n",
    "\n",
    "        self.sample_list = []\n",
    "        for series_uid in self.series_list:\n",
    "            index_count, positive_indexes = getCtSampleSize(series_uid)\n",
    "\n",
    "            if self.fullCt_bool:\n",
    "                self.sample_list += [\n",
    "                    (series_uid, slice_ndx) for slice_ndx in range(index_count)\n",
    "                ]\n",
    "            else:\n",
    "                self.sample_list += [\n",
    "                    (series_uid, slice_ndx) for slice_ndx in positive_indexes\n",
    "                ]\n",
    "\n",
    "        self.candidateInfo_list = getCandidateInfoList()\n",
    "\n",
    "        series_set = set(self.series_list)\n",
    "        self.candidateInfo_list = [\n",
    "            cit for cit in self.candidateInfo_list if cit.series_uid in series_set\n",
    "        ]\n",
    "\n",
    "        self.pos_list = [nt for nt in self.candidateInfo_list if nt.isNodule_bool]\n",
    "\n",
    "        log.info(\n",
    "            \"{!r}: {} {} series, {} slices, {} nodules\".format(\n",
    "                self,\n",
    "                len(self.series_list),\n",
    "                {None: \"general\", True: \"validation\", False: \"training\"}[isValSet_bool],\n",
    "                len(self.sample_list),\n",
    "                len(self.pos_list),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
    "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
    "\n",
    "    def getitem_fullSlice(self, series_uid, slice_ndx):\n",
    "        ct = getCt(series_uid)\n",
    "        ct_t = torch.zeros((self.contextSlices_count * 2 + 1, 512, 512))\n",
    "\n",
    "        start_ndx = slice_ndx - self.contextSlices_count\n",
    "        end_ndx = slice_ndx + self.contextSlices_count + 1\n",
    "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
    "            context_ndx = max(context_ndx, 0)\n",
    "            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n",
    "            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
    "        # The upper bound nukes any weird hotspots and clamps bone down\n",
    "        ct_t.clamp_(-1000, 1000)\n",
    "\n",
    "        pos_t = torch.from_numpy(ct.positive_mask[slice_ndx]).unsqueeze(0)\n",
    "\n",
    "        return ct_t, pos_t, ct.series_uid, slice_ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 22:31:45,019 INFO     pid:55736 __main__:051:__init__ <__main__.Luna2dSegmentationDataset object at 0x704ae5557610>: 89 general series, 971 slices, 112 nodules\n"
     ]
    }
   ],
   "source": [
    "ds = Luna2dSegmentationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 512, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util import enumerateWithEstimate\n",
    "from logconf import logging\n",
    "from unet import UNet\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs[\"in_channels\"])\n",
    "        self.unet = UNet(**kwargs)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        init_set = {\n",
    "            nn.Conv2d,\n",
    "            nn.Conv3d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.ConvTranspose3d,\n",
    "            nn.Linear,\n",
    "        }\n",
    "        for m in self.modules():\n",
    "            if type(m) in init_set:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, mode=\"fan_out\", nonlinearity=\"relu\", a=0\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(\n",
    "                        m.weight.data\n",
    "                    )\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "        # nn.init.constant_(self.unet.last.bias, -4)\n",
    "        # nn.init.constant_(self.unet.last.bias, 4)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.unet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output\n",
    "\n",
    "\n",
    "class SegmentationAugmentation(nn.Module):\n",
    "    def __init__(self, flip=None, offset=None, scale=None, rotate=None, noise=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flip = flip\n",
    "        self.offset = offset\n",
    "        self.scale = scale\n",
    "        self.rotate = rotate\n",
    "        self.noise = noise\n",
    "\n",
    "    def forward(self, input_g, label_g):\n",
    "        transform_t = self._build2dTransformMatrix()\n",
    "        transform_t = transform_t.expand(input_g.shape[0], -1, -1)\n",
    "        transform_t = transform_t.to(input_g.device, torch.float32)\n",
    "        affine_t = F.affine_grid(\n",
    "            transform_t[:, :2], input_g.size(), align_corners=False\n",
    "        )\n",
    "\n",
    "        augmented_input_g = F.grid_sample(\n",
    "            input_g, affine_t, padding_mode=\"border\", align_corners=False\n",
    "        )\n",
    "        augmented_label_g = F.grid_sample(\n",
    "            label_g.to(torch.float32),\n",
    "            affine_t,\n",
    "            padding_mode=\"border\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if self.noise:\n",
    "            noise_t = torch.randn_like(augmented_input_g)\n",
    "            noise_t *= self.noise\n",
    "\n",
    "            augmented_input_g += noise_t\n",
    "\n",
    "        return augmented_input_g, augmented_label_g > 0.5\n",
    "\n",
    "    def _build2dTransformMatrix(self):\n",
    "        transform_t = torch.eye(3)\n",
    "\n",
    "        for i in range(2):\n",
    "            if self.flip:\n",
    "                if random.random() > 0.5:\n",
    "                    transform_t[i, i] *= -1\n",
    "\n",
    "            if self.offset:\n",
    "                offset_float = self.offset\n",
    "                random_float = random.random() * 2 - 1\n",
    "                transform_t[2, i] = offset_float * random_float\n",
    "\n",
    "            if self.scale:\n",
    "                scale_float = self.scale\n",
    "                random_float = random.random() * 2 - 1\n",
    "                transform_t[i, i] *= 1.0 + scale_float * random_float\n",
    "\n",
    "        if self.rotate:\n",
    "            angle_rad = random.random() * math.pi * 2\n",
    "            s = math.sin(angle_rad)\n",
    "            c = math.cos(angle_rad)\n",
    "\n",
    "            rotation_t = torch.tensor([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
    "\n",
    "            transform_t @= rotation_t\n",
    "\n",
    "        return transform_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_dict = {}\n",
    "\n",
    "augmentation_dict[\"flip\"] = True\n",
    "\n",
    "augmentation_dict[\"offset\"] = 0.03\n",
    "\n",
    "augmentation_dict[\"scale\"] = 0.2\n",
    "\n",
    "augmentation_dict[\"rotate\"] = True\n",
    "\n",
    "augmentation_dict[\"noise\"] = 25.0\n",
    "\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 22:47:17,959 INFO     pid:55736 __main__:015:initModel Using CUDA; 1 devices.\n"
     ]
    }
   ],
   "source": [
    "def initModel():\n",
    "    segmentation_model = UNetWrapper(\n",
    "        in_channels=7,\n",
    "        n_classes=1,\n",
    "        depth=3,\n",
    "        wf=4,\n",
    "        padding=True,\n",
    "        batch_norm=True,\n",
    "        up_mode=\"upconv\",\n",
    "    )\n",
    "\n",
    "    augmentation_model = SegmentationAugmentation(augmentation_dict)\n",
    "\n",
    "    if use_cuda:\n",
    "        log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            segmentation_model = nn.DataParallel(segmentation_model)\n",
    "            augmentation_model = nn.DataParallel(augmentation_model)\n",
    "        segmentation_model = segmentation_model.to(device)\n",
    "        augmentation_model = augmentation_model.to(device)\n",
    "\n",
    "    return segmentation_model, augmentation_model\n",
    "\n",
    "\n",
    "segmentation_model, augmentation_model = initModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 8\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLuna2dSegmentationDataset(Luna2dSegmentationDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.ratio_int = 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return 300000\n",
    "\n",
    "    def shuffleSamples(self):\n",
    "        random.shuffle(self.candidateInfo_list)\n",
    "        random.shuffle(self.pos_list)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n",
    "        return self.getitem_trainingCrop(candidateInfo_tup)\n",
    "\n",
    "    def getitem_trainingCrop(self, candidateInfo_tup):\n",
    "        ct_a, pos_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            (7, 96, 96),\n",
    "        )\n",
    "        pos_a = pos_a[3:4]\n",
    "\n",
    "        row_offset = random.randrange(0, 32)\n",
    "        col_offset = random.randrange(0, 32)\n",
    "        ct_t = torch.from_numpy(\n",
    "            ct_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.float32)\n",
    "        pos_t = torch.from_numpy(\n",
    "            pos_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.long)\n",
    "\n",
    "        slice_ndx = center_irc.index\n",
    "\n",
    "        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initOptimizer():\n",
    "    return Adam(segmentation_model.parameters())\n",
    "    # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "\n",
    "optimizer = initOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initTrainDl():\n",
    "    train_ds = TrainingLuna2dSegmentationDataset(\n",
    "        val_stride=10,\n",
    "        isValSet_bool=False,\n",
    "        contextSlices_count=3,\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "    if use_cuda:\n",
    "        batch_size *= torch.cuda.device_count()\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "    )\n",
    "\n",
    "    return train_dl\n",
    "\n",
    "\n",
    "def initValDl():\n",
    "    val_ds = Luna2dSegmentationDataset(\n",
    "        val_stride=10,\n",
    "        isValSet_bool=True,\n",
    "        contextSlices_count=3,\n",
    "    )\n",
    "\n",
    "    batch_size = batch_size\n",
    "    if use_cuda:\n",
    "        batch_size *= torch.cuda.device_count()\n",
    "\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "    )\n",
    "\n",
    "    return val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:01:08,759 INFO     pid:55736 __main__:051:__init__ <__main__.TrainingLuna2dSegmentationDataset object at 0x704af1ff69d0>: 80 training series, 890 slices, 101 nodules\n"
     ]
    }
   ],
   "source": [
    "train_ds = TrainingLuna2dSegmentationDataset(\n",
    "    val_stride=10,\n",
    "    isValSet_bool=False,\n",
    "    contextSlices_count=3,\n",
    ")\n",
    "\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_cuda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:03:05,657 INFO     pid:55736 __main__:051:__init__ <__main__.Luna2dSegmentationDataset object at 0x704af1faf2d0>: 9 validation series, 81 slices, 11 nodules\n"
     ]
    }
   ],
   "source": [
    "val_ds = Luna2dSegmentationDataset(\n",
    "    val_stride=10,\n",
    "    isValSet_bool=True,\n",
    "    contextSlices_count=3,\n",
    ")\n",
    "\n",
    "if use_cuda:\n",
    "    batch_size *= torch.cuda.device_count()\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_cuda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceLoss(prediction_g, label_g, epsilon=1):\n",
    "    diceLabel_g = label_g.sum(dim=[1, 2, 3])\n",
    "    dicePrediction_g = prediction_g.sum(dim=[1, 2, 3])\n",
    "    diceCorrect_g = (prediction_g * label_g).sum(dim=[1, 2, 3])\n",
    "\n",
    "    diceRatio_g = (2 * diceCorrect_g + epsilon) / (\n",
    "        dicePrediction_g + diceLabel_g + epsilon\n",
    "    )\n",
    "\n",
    "    return 1 - diceRatio_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for computeClassificationLoss and logMetrics to index into metrics_t/metrics_a\n",
    "# METRICS_LABEL_NDX = 0\n",
    "METRICS_LOSS_NDX = 1\n",
    "# METRICS_FN_LOSS_NDX = 2\n",
    "# METRICS_ALL_LOSS_NDX = 3\n",
    "\n",
    "# METRICS_PTP_NDX = 4\n",
    "# METRICS_PFN_NDX = 5\n",
    "# METRICS_MFP_NDX = 6\n",
    "METRICS_TP_NDX = 7\n",
    "METRICS_FN_NDX = 8\n",
    "METRICS_FP_NDX = 9\n",
    "\n",
    "METRICS_SIZE = 10\n",
    "\n",
    "\n",
    "def computeBatchLoss(\n",
    "    batch_ndx, batch_tup, batch_size, metrics_g, classificationThreshold=0.5\n",
    "):\n",
    "    input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
    "\n",
    "    input_g = input_t.to(device, non_blocking=True)\n",
    "    label_g = label_t.to(device, non_blocking=True)\n",
    "\n",
    "    if segmentation_model.training and augmentation_dict:\n",
    "        input_g, label_g = augmentation_model(input_g, label_g)\n",
    "\n",
    "    prediction_g = segmentation_model(input_g)\n",
    "\n",
    "    diceLoss_g = diceLoss(prediction_g, label_g)\n",
    "    fnLoss_g = diceLoss(prediction_g * label_g, label_g)\n",
    "\n",
    "    start_ndx = batch_ndx * batch_size\n",
    "    end_ndx = start_ndx + input_t.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictionBool_g = (prediction_g[:, 0:1] > classificationThreshold).to(\n",
    "            torch.float32\n",
    "        )\n",
    "\n",
    "        tp = (predictionBool_g * label_g).sum(dim=[1, 2, 3])\n",
    "        fn = ((1 - predictionBool_g) * label_g).sum(dim=[1, 2, 3])\n",
    "        fp = (predictionBool_g * (~label_g)).sum(dim=[1, 2, 3])\n",
    "\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
    "        metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "        metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "        metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "\n",
    "    return diceLoss_g.mean() + fnLoss_g.mean() * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTraining(epoch_ndx, train_dl):\n",
    "    totalTrainingSamples_count = 0\n",
    "    trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=device)\n",
    "    segmentation_model.train()\n",
    "    train_dl.dataset.shuffleSamples()\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, batch_tup in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_var = computeBatchLoss(\n",
    "            batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g\n",
    "        )\n",
    "        loss_var.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    totalTrainingSamples_count += trnMetrics_g.size(1)\n",
    "\n",
    "    return trnMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doValidation(epoch_ndx, val_dl):\n",
    "    with torch.no_grad():\n",
    "        valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=device)\n",
    "        segmentation_model.eval()\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            computeBatchLoss(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "    return valMetrics_g.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 23:19:36,241 INFO     pid:55736 __main__:004:<module> Epoch 1 of 1, 9375/3 batches of size 32*1\n",
      "2024-11-01 23:19:36,285 WARNING  pid:55736 util:219:enumerateWithEstimate E1 Training ----/9375, starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_ndx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m batches of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      6\u001b[0m             epoch_ndx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     trnMetrics_t \u001b[38;5;241m=\u001b[39m doTraining(epoch_ndx, train_dl)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# logMetrics(epoch_ndx, \"trn\", trnMetrics_t)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_ndx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch_ndx \u001b[38;5;241m%\u001b[39m validation_cadence \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# if validation is wanted\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 12\u001b[0m, in \u001b[0;36mdoTraining\u001b[0;34m(epoch_ndx, train_dl)\u001b[0m\n\u001b[1;32m      5\u001b[0m train_dl\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mshuffleSamples()\n\u001b[1;32m      7\u001b[0m batch_iter \u001b[38;5;241m=\u001b[39m enumerateWithEstimate(\n\u001b[1;32m      8\u001b[0m     train_dl,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_ndx),\n\u001b[1;32m     10\u001b[0m     start_ndx\u001b[38;5;241m=\u001b[39mtrain_dl\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_ndx, batch_tup \u001b[38;5;129;01min\u001b[39;00m batch_iter:\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m     loss_var \u001b[38;5;241m=\u001b[39m computeBatchLoss(\n\u001b[1;32m     16\u001b[0m         batch_ndx, batch_tup, train_dl\u001b[38;5;241m.\u001b[39mbatch_size, trnMetrics_g\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/other/ML/lesson13/util.py:224\u001b[0m, in \u001b[0;36menumerateWithEstimate\u001b[0;34m(iter, desc_str, start_ndx, print_ndx, backoff, iter_len)\u001b[0m\n\u001b[1;32m    219\u001b[0m log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ----/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, starting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m     desc_str,\n\u001b[1;32m    221\u001b[0m     iter_len,\n\u001b[1;32m    222\u001b[0m ))\n\u001b[1;32m    223\u001b[0m start_ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (current_ndx, item) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (current_ndx, item)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_ndx \u001b[38;5;241m==\u001b[39m print_ndx:\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# ... <1>\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/Software/anaconda3/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 0.0\n",
    "validation_cadence = 5\n",
    "for epoch_ndx in range(1, epochs + 1):\n",
    "    log.info(\n",
    "        \"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "            epoch_ndx,\n",
    "            epochs,\n",
    "            len(train_dl),\n",
    "            len(val_dl),\n",
    "            batch_size,\n",
    "            (torch.cuda.device_count() if use_cuda else 1),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trnMetrics_t = doTraining(epoch_ndx, train_dl)\n",
    "    # logMetrics(epoch_ndx, \"trn\", trnMetrics_t)\n",
    "\n",
    "    if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
    "        # if validation is wanted\n",
    "        valMetrics_t = doValidation(epoch_ndx, val_dl)\n",
    "        # score = logMetrics(epoch_ndx, \"val\", valMetrics_t)\n",
    "        # best_score = max(score, best_score)\n",
    "\n",
    "        # saveModel(\"seg\", epoch_ndx, score == best_score)\n",
    "\n",
    "        # logImages(epoch_ndx, \"trn\", train_dl)\n",
    "        # logImages(epoch_ndx, \"val\", val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
